{
  "implemented_features": [
    "A"
  ],
  "qa": [
    {
      "question": "Summarize the main contribution of TinyLoRA in 3 bullets.",
      "answer": "Here are the main contributions of TinyLoRA in 3 bullets:\n\n* TinyLoRA is a technique for reducing trainable parameters in models by replacing a matrix with a low-dimensional trainable vector projected through a fixed random tensor, enabling training with as few as 1 changed parameter [Source: TinyLoRA_2602.04118v1.pdf, Chunk 13].\n* It achieves significant parameter efficiency by scaling trainable parameters to O(nmu), where n is the number of layers, m is the number of modules, and u is the TinyLoRA projection dimension, which can be further reduced to a single parameter when all modules share weights [Source: TinyLoRA_2602.04118v1.pdf, Chunk 13].\n* TinyLoRA demonstrates strong performance in extremely low-parameter regimes, achieving high accuracy on benchmarks like GSM8K, MATH, AIME, and AMC with as few as 1-196 trainable parameters, and shows that RL makes more information-dense updates than SFT, making it suitable for tiny update sizes [Source: TinyLoRA_2602.04118v1.pdf, Chunk 5].",
      "citations": [
        {
          "source": "TinyLoRA_2602.04118v1.pdf",
          "locator": "Chunk 13",
          "snippet": "TinyLoRA is a technique for reducing trainable parameters in models by replacing the $r \\times r$ matrix $R$ with a low-dimensional trainable vector $v \\in R^u$ projected through a fixed random tensor"
        },
        {
          "source": "TinyLoRA_2602.04118v1.pdf",
          "locator": "Chunk 5",
          "snippet": "Figure 2 Using Qwen2.5-7B-Instruct as a base model, SFT works best with larger update sizes of at least 1M parameters.\nSince models experience such little performance degradation at 104 parameters, we"
        }
      ]
    },
    {
      "question": "What are the key assumptions or limitations of TinyLoRA?",
      "answer": "I don't have enough information in the uploaded documents to answer that question.",
      "citations": [
        {
          "source": "TinyLoRA_2602.04118v1.pdf",
          "locator": "Chunk 13",
          "snippet": "TinyLoRA is a technique for reducing trainable parameters in models by replacing the $r \\times r$ matrix $R$ with a low-dimensional trainable vector $v \\in R^u$ projected through a fixed random tensor"
        }
      ]
    },
    {
      "question": "Give one concrete numeric or experimental detail from TinyLoRA and cite it.",
      "answer": "One concrete numeric detail from TinyLoRA is that it achieves 91% accuracy on GSM8K while finetuning just 13 total parameters, which is an update size of just 26 bytes [Source: TinyLoRA_2602.04118v1.pdf, Chunk 5].",
      "citations": [
        {
          "source": "TinyLoRA_2602.04118v1.pdf",
          "locator": "Chunk 5",
          "snippet": "Figure 2 Using Qwen2.5-7B-Instruct as a base model, SFT works best with larger update sizes of at least 1M parameters.\nSince models experience such little performance degradation at 104 parameters, we"
        }
      ]
    }
  ],
  "demo": {
    "pdf_used": "sample_docs/TinyLoRA_2602.04118v1.pdf",
    "num_questions": 5,
    "retrieval_top_k": 5,
    "refusal_tests": [
      {
        "question": "What is the CEO's phone number?",
        "answer": "I don't have enough information in the uploaded documents to answer that question.",
        "refused_correctly": true,
        "hallucinated_citations": false
      },
      {
        "question": "What was the GDP of France in 2019?",
        "answer": "I don't have enough information in the uploaded documents to answer that question.",
        "refused_correctly": true,
        "hallucinated_citations": false
      }
    ]
  }
}