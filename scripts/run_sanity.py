"""
Sanity check script for the Agentic RAG Chatbot.

It performs a minimal end-to-end RAG flow and writes
the required `artifacts/sanity_output.json`.
"""
import json
import os
import shutil

# ---------------------------------------------------------------------------
# Import pipeline modules
# ---------------------------------------------------------------------------
from src.app.ingestion.pipeline import run_ingestion_pipeline
from src.app.retrieval.retriever import get_retriever, retrieve
from src.app.generation.generator import generate_answer
from src.app.memory import process_memory
from src.app.logger import get_logger

logger = get_logger(__name__)

# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------
# Use the sample PDF
PDF_PATH = "sample_docs/TinyLoRA_2602.04118v1.pdf"

# Temporary vector store ‚Äî cleaned up at the end.
SANITY_DB = "sanity_chroma_db"

# Output path that verify_output.py and sanity_check.sh expect.
OUTPUT_PATH = "artifacts/sanity_output.json"

# ---------------------------------------------------------------------------
# Evaluation Questions (from EVAL_QUESTIONS.md)
# ---------------------------------------------------------------------------
# Group 1: Answerable ‚Äî these go into the "qa" array (citations required)
ANSWERABLE_QUESTIONS = [
    "Summarize the main contribution of TinyLoRA in 3 bullets.",
    "What are the key assumptions or limitations of TinyLoRA?",
    "Give one concrete numeric or experimental detail from TinyLoRA and cite it.",
]

# Group 2: Unanswerable ‚Äî these go into "demo.refusal_tests"
UNANSWERABLE_QUESTIONS = [
    "What is the CEO's phone number?",
    "What was the GDP of France in 2019?",
]


def run_sanity():
    """
    Execute a minimal end-to-end RAG flow and write the output JSON.

    Flow:
        1. Ingest PDF ‚Üí parse, chunk, index into ChromaDB
        2. For each question ‚Üí retrieve relevant chunks ‚Üí generate answer
        3. Format results into the schema verify_output.py expects
        4. Write artifacts/sanity_output.json
        5. Clean up temp vector store
    """
    logger.info("=" * 60)
    logger.info("üîç SANITY CHECK ‚Äî Starting end-to-end RAG flow")
    logger.info("=" * 60)

    # ------------------------------------------------------------------
    # Step 1: Ingest the PDF
    # ------------------------------------------------------------------
    # This calls parse ‚Üí chunk ‚Üí index, the full Feature A pipeline.
    logger.info(f"üì• Ingesting: {PDF_PATH}")
    vectorstore = run_ingestion_pipeline(
        PDF_PATH,
        persist_dir=SANITY_DB,
        extract_images=True,
    )
    logger.info("‚úÖ Ingestion complete")

    # ------------------------------------------------------------------
    # Step 2: Build the retriever
    # ------------------------------------------------------------------
    retriever = get_retriever(vectorstore, top_k=5)

    # ------------------------------------------------------------------
    # Step 3: Ask questions and collect results
    # ------------------------------------------------------------------
    qa_results = []

    for question in ANSWERABLE_QUESTIONS:
        logger.info(f"\nüìù Question: {question}")

        # Retrieve relevant chunks from ChromaDB
        docs = retrieve(retriever, question)

        # Generate grounded answer with citations
        result = generate_answer(question, docs)

        # ----------------------------------------------------------
        # Step 4: Format citations
        # ----------------------------------------------------------
        formatted_citations = []
        for c in result["citations"]:
            formatted_citations.append({
                "source": c["source"],
                "locator": f"Chunk {c['chunk_id']}",
                "snippet": c["snippet"] if c["snippet"] else docs[0].page_content[:200],
            })

        # If the LLM didn't produce citation markers but we have docs,
        # add a fallback citation.
        # This can happen if the LLM forgets the [Source: ...] format.
        if not formatted_citations and docs:
            formatted_citations.append({
                "source": docs[0].metadata.get("source", "unknown"),
                "locator": f"Chunk {docs[0].metadata.get('chunk_id', 0)}",
                "snippet": docs[0].page_content[:200],
            })

        qa_results.append({
            "question": question,
            "answer": result["answer"],
            "citations": formatted_citations,
        })

        logger.info(f"   ‚úÖ Answer generated with {len(formatted_citations)} citation(s)")

    # ------------------------------------------------------------------
    # Step 5: Run unanswerable questions (refusal tests)
    # ------------------------------------------------------------------
    # These test that the LLM gracefully refuses to answer questions
    # not covered by the uploaded documents ‚Äî no hallucinations.
    refusal_results = []

    for question in UNANSWERABLE_QUESTIONS:
        logger.info(f"\nüö´ Refusal test: {question}")

        docs = retrieve(retriever, question)
        result = generate_answer(question, docs)

        # Check if the LLM correctly refused
        answer_lower = result["answer"].lower()
        refused = any(phrase in answer_lower for phrase in [
            "i don't have", "i cannot find", "not found",
            "no relevant", "not covered", "cannot answer",
            "not mentioned", "no information", "don't have enough",
        ])

        refusal_results.append({
            "question": question,
            "answer": result["answer"],
            "refused_correctly": refused,
            "hallucinated_citations": len(result["citations"]) > 0,
        })

        status = "‚úÖ Refused" if refused else "‚ö†Ô∏è May have hallucinated"
        logger.info(f"   {status}")

    # ------------------------------------------------------------------
    # Step 5.1: Memory extraction ‚Äî Feature B
    # ------------------------------------------------------------------
    logger.info("\nüß† Running memory extraction tests...")

    memory_test_turns = [
        (
            "I prefer weekly summaries on Mondays.",
            "Noted! I'll remember that you prefer weekly summaries delivered on Mondays.",
        ),
        (
            "I'm a Project Finance Analyst at a large asset management firm.",
            "Thanks for sharing! That context helps me tailor my responses to your domain.",
        ),
    ]

    for user_msg, assistant_resp in memory_test_turns:
        logger.info(f"  üí¨ User: {user_msg}")
        mem_result = process_memory(user_msg, assistant_resp)
        logger.info(f"  üß† Saved={mem_result['memory_saved']}, confidence={mem_result['confidence']:.2f}")

    # Build memory_writes from what was actually written to the files.
    # verify_output.py expects: [{target: "USER"|"COMPANY", summary: "..."}]
    memory_writes = []

    from src.app.memory import read_memory
    from src.app.config import USER_MEMORY_PATH, COMPANY_MEMORY_PATH

    user_mem_content = read_memory(USER_MEMORY_PATH)
    company_mem_content = read_memory(COMPANY_MEMORY_PATH)

    # Extract bullet-point facts from the memory files
    for line in user_mem_content.splitlines():
        line = line.strip()
        if line.startswith("- "):
            memory_writes.append({
                "target": "USER",
                "summary": line[2:].strip(),
            })

    for line in company_mem_content.splitlines():
        line = line.strip()
        if line.startswith("- "):
            memory_writes.append({
                "target": "COMPANY",
                "summary": line[2:].strip(),
            })

    logger.info(f"  üìù Total memory_writes for output: {len(memory_writes)}")

    # ------------------------------------------------------------------
    # Step 6: Build the output JSON
    # ------------------------------------------------------------------
    output = {
        "implemented_features": ["A", "B"],
        "qa": qa_results,
        "demo": {
            "pdf_used": PDF_PATH,
            "num_questions": len(ANSWERABLE_QUESTIONS) + len(UNANSWERABLE_QUESTIONS),
            "retrieval_top_k": 5,
            "refusal_tests": refusal_results,
            "memory_writes": memory_writes,
        },
    }

    # ------------------------------------------------------------------
    # Step 7: Write the output file
    # ------------------------------------------------------------------
    os.makedirs("artifacts", exist_ok=True)
    with open(OUTPUT_PATH, "w") as f:
        json.dump(output, f, indent=2)

    logger.info(f"\nüíæ Output written to: {OUTPUT_PATH}")

    # ------------------------------------------------------------------
    # Step 8: Cleanup temp vector store
    # ------------------------------------------------------------------
    # Don't want sanity_chroma_db committed to the repo.
    shutil.rmtree(SANITY_DB, ignore_errors=True)
    logger.info("üßπ Temp vector store cleaned up")

    logger.info("\n‚úÖ SANITY CHECK PASSED")
    return output


if __name__ == "__main__":
    run_sanity()
